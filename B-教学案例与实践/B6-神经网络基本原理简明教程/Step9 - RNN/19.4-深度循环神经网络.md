<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 19.4 深度循环神经网络

前面的几个例子中，单独看每一时刻的网络结构，其实都是由“输入层->隐层->输出层”所组成的，这与我们在DNN中学到的单隐层的知识一样，由于输入层不算做网络的一层，输出层是必须具备的，所以网络只有一个隐层。我们知道单隐层的能力是有限的，所以人们会在DNN中使用更深（更多隐层）的网络来解决复杂的问题。

在RNN中，会有同样的需求，要求每一时刻的网络是由多个隐层组成。比如下图为两个隐层的循环神经网络，用于解决和19.3节中的同样的问题：

<img src="../Images/19/deep_rnn_net.png"/>
<center>图一：</center>

注意图一中最左侧的两个隐藏状态s1和s2是同时展开为右侧的图的，
这样的循环神经网络称为深度循环神经网络，它可以具备比单隐层的“简单”循环神经网络更强大的能力。



正向计算


对第一个时间步：
$$
h_1 = x \cdot U
$$
$$
s_1 = tanh(h_1)
$$
$$
h_2 = s_1 \cdot Q
$$
$$
s_2 = tanh(h_2)
$$

对后面的时间步：
$$
h_1 = x \cdot U + s_1 \cdot W_1
$$

$$
h_2 = s_1 \cdot Q + s_2 \cdot W_2
$$

对于最后一个时间步：
$$
z = s_2 \cdot V
$$
$$
a = Softmax(z)
$$
$$
loss = -y \cdot \ln a
$$

注意并不是所有的循环神经网络都只在最后一个时间步有监督学习信号（计算loss值），而是我们这个问题只需要这样。在19.2节中的例子就是需要在每一个时间步都要计算loss值的。

反向传播

反向传播部分和前面章节的内容大致相似，我们只把几个关键步骤直接列出来，不做具体推导：

对于最后一个时间步：
$$
\frac{\partial loss}{\partial z} = a-y \rightarrow dz
$$
对于其它时间步：
$$
dz = 0
$$
对于最后一个时间步，下式中$dh_{t+1}$为0；对于其他时间步，dz为0:
$$
\frac{\partial loss}{\partial h_2} = (dz \cdot V^T + dh_{t+1} \cdot W2^T) \odot tanh'(s_2)
$$



<img src="../Images/19/deep_rnn_loss.png"/>
<img src="../Images/19/deep_rnn_result.png"/>


参数量计算

对于19.3节中的单层RNN，参数配置如下：

|参数|尺寸|数量|
|---|---|---|
|U|26x8|208|
|V|8x6|48|
|W|8x8|64|
|Total||320|

对于两层的RNN来说，参数配置如下：

|参数|尺寸|数量|
|---|---|---|
|U|26x4|104|
|Q|4x4|26|
|V|4x6|24|
|W1|4x4|16|
|W2|4x4|16|
|Total||156|

通过上面两张表的比较，可以看出，单层RNN的参数总量为320个，双层RNN（代表了深度RNN）的参数总量为156个，只有前者的一半，但是可以达到同样的效果，甚至更好的效果。


```
129:285220:0.0005 loss=0.924615, acc=0.587000
save parameters...
130:287414:0.0005 loss=0.929041, acc=0.543000
...
149:329100:0.0005 loss=0.935588, acc=0.586000
correctness=5801/8759=0.6622902157780568
```

```
...
69:153580:0.001 loss=0.915009, acc=0.684000
save parameters...
70:155774:0.001 loss=0.973193, acc=0.669000
...
99:219400:0.0005 loss=0.935686, acc=0.698000
correctness=5867/8759=0.6698253225254024
```